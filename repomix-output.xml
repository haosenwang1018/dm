This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: ./*.py
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
app.py
config.py
data_utils.py
milvus_utils.py
models.py
preprocess.py
rag_core.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="app.py">
import streamlit as st
import time
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = './hf_cache' 


# Import functions and config from other modules
from config import (
    DATA_FILE, EMBEDDING_MODEL_NAME, GENERATION_MODEL_NAME, TOP_K,
    MAX_ARTICLES_TO_INDEX, MILVUS_LITE_DATA_PATH, COLLECTION_NAME,
    id_to_doc_map # Import the global map
)
from data_utils import load_data
from models import load_embedding_model, load_generation_model
# Import the new Milvus Lite functions
from milvus_utils import get_milvus_client, setup_milvus_collection, index_data_if_needed, search_similar_documents
from rag_core import generate_answer

# --- Streamlit UI è®¾ç½® ---
st.set_page_config(layout="wide")
st.title("ğŸ“„ åŒ»ç–— RAG ç³»ç»Ÿ (Milvus Lite)")
st.markdown(f"ä½¿ç”¨ Milvus Lite, `{EMBEDDING_MODEL_NAME}`, å’Œ `{GENERATION_MODEL_NAME}`ã€‚")

# --- åˆå§‹åŒ–ä¸ç¼“å­˜ ---
# è·å– Milvus Lite å®¢æˆ·ç«¯ (å¦‚æœæœªç¼“å­˜åˆ™åˆå§‹åŒ–)
milvus_client = get_milvus_client()

if milvus_client:
    # è®¾ç½® collection (å¦‚æœæœªç¼“å­˜åˆ™åˆ›å»º/åŠ è½½ç´¢å¼•)
    collection_is_ready = setup_milvus_collection(milvus_client)

    # åŠ è½½æ¨¡å‹ (ç¼“å­˜)
    embedding_model = load_embedding_model(EMBEDDING_MODEL_NAME)
    generation_model, tokenizer = load_generation_model(GENERATION_MODEL_NAME)

    # æ£€æŸ¥æ‰€æœ‰ç»„ä»¶æ˜¯å¦æˆåŠŸåŠ è½½
    models_loaded = embedding_model and generation_model and tokenizer

    if collection_is_ready and models_loaded:
        # åŠ è½½æ•°æ® (æœªç¼“å­˜)
        pubmed_data = load_data(DATA_FILE)

        # å¦‚æœéœ€è¦åˆ™ç´¢å¼•æ•°æ® (è¿™ä¼šå¡«å…… id_to_doc_map)
        if pubmed_data:
            indexing_successful = index_data_if_needed(milvus_client, pubmed_data, embedding_model)
        else:
            st.warning(f"æ— æ³•ä» {DATA_FILE} åŠ è½½æ•°æ®ã€‚è·³è¿‡ç´¢å¼•ã€‚")
            indexing_successful = False # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œåˆ™è§†ä¸ºä¸æˆåŠŸ

        st.divider()

        # --- RAG äº¤äº’éƒ¨åˆ† ---
        if not indexing_successful and not id_to_doc_map:
             st.error("æ•°æ®ç´¢å¼•å¤±è´¥æˆ–ä¸å®Œæ•´ï¼Œä¸”æ²¡æœ‰æ–‡æ¡£æ˜ å°„ã€‚RAG åŠŸèƒ½å·²ç¦ç”¨ã€‚")
        else:
            query = st.text_input("è¯·æå‡ºå…³äºå·²ç´¢å¼•åŒ»ç–—æ–‡ç« çš„é—®é¢˜:", key="query_input")

            if st.button("è·å–ç­”æ¡ˆ", key="submit_button") and query:
                start_time = time.time()

                # 1. æœç´¢ Milvus Lite
                with st.spinner("æ­£åœ¨æœç´¢ç›¸å…³æ–‡æ¡£..."):
                    retrieved_ids, distances = search_similar_documents(milvus_client, query, embedding_model)

                if not retrieved_ids:
                    st.warning("åœ¨æ•°æ®åº“ä¸­æ‰¾ä¸åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
                else:
                    # 2. ä»æ˜ å°„ä¸­æ£€ç´¢ä¸Šä¸‹æ–‡
                    retrieved_docs = [id_to_doc_map[id] for id in retrieved_ids if id in id_to_doc_map]

                    if not retrieved_docs:
                         st.error("æ£€ç´¢åˆ°çš„ ID æ— æ³•æ˜ å°„åˆ°åŠ è½½çš„æ–‡æ¡£ã€‚è¯·æ£€æŸ¥æ˜ å°„é€»è¾‘ã€‚")
                    else:
                        st.subheader("æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æ¡£:")
                        for i, doc in enumerate(retrieved_docs):
                            # å¦‚æœè·ç¦»å¯ç”¨åˆ™æ˜¾ç¤ºï¼Œå¦åˆ™åªæ˜¾ç¤º ID
                            dist_str = f", è·ç¦»: {distances[i]:.4f}" if distances else ""
                            with st.expander(f"æ–‡æ¡£ {i+1} (ID: {retrieved_ids[i]}{dist_str}) - {doc['title'][:60]}"):
                                st.write(f"**æ ‡é¢˜:** {doc['title']}")
                                st.write(f"**æ‘˜è¦:** {doc['abstract']}") # å‡è®¾ 'abstract' å­˜å‚¨çš„æ˜¯æ–‡æœ¬å—

                        st.divider()

                        # 3. ç”Ÿæˆç­”æ¡ˆ
                        st.subheader("ç”Ÿæˆçš„ç­”æ¡ˆ:")
                        with st.spinner("æ­£åœ¨æ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ..."):
                            answer = generate_answer(query, retrieved_docs, generation_model, tokenizer)
                            st.write(answer)

                end_time = time.time()
                st.info(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")

    else:
        st.error("åŠ è½½æ¨¡å‹æˆ–è®¾ç½® Milvus Lite collection å¤±è´¥ã€‚è¯·æ£€æŸ¥æ—¥å¿—å’Œé…ç½®ã€‚")
else:
    st.error("åˆå§‹åŒ– Milvus Lite å®¢æˆ·ç«¯å¤±è´¥ã€‚è¯·æ£€æŸ¥æ—¥å¿—ã€‚")


# --- é¡µè„š/ä¿¡æ¯ä¾§è¾¹æ  ---
st.sidebar.header("ç³»ç»Ÿé…ç½®")
st.sidebar.markdown(f"**å‘é‡å­˜å‚¨:** Milvus Lite")
st.sidebar.markdown(f"**æ•°æ®è·¯å¾„:** `{MILVUS_LITE_DATA_PATH}`")
st.sidebar.markdown(f"**Collection:** `{COLLECTION_NAME}`")
st.sidebar.markdown(f"**æ•°æ®æ–‡ä»¶:** `{DATA_FILE}`")
st.sidebar.markdown(f"**åµŒå…¥æ¨¡å‹:** `{EMBEDDING_MODEL_NAME}`")
st.sidebar.markdown(f"**ç”Ÿæˆæ¨¡å‹:** `{GENERATION_MODEL_NAME}`")
st.sidebar.markdown(f"**æœ€å¤§ç´¢å¼•æ•°:** `{MAX_ARTICLES_TO_INDEX}`")
st.sidebar.markdown(f"**æ£€ç´¢ Top K:** `{TOP_K}`")
</file>

<file path="config.py">
# Milvus Lite Configuration
MILVUS_LITE_DATA_PATH = "./milvus_lite_data.db" # Path to store Milvus Lite data
COLLECTION_NAME = "medical_rag_lite" # Use a different name if needed

# Data Configuration
DATA_FILE = "./data/processed_data.json"

# Model Configuration
# Example: 'all-MiniLM-L6-v2' (dim 384), 'thenlper/gte-large' (dim 1024)
EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'
GENERATION_MODEL_NAME = "Qwen/Qwen2.5-0.5B"
EMBEDDING_DIM = 384 # Must match EMBEDDING_MODEL_NAME

# Indexing and Search Parameters
MAX_ARTICLES_TO_INDEX = 500
TOP_K = 3
# Milvus index parameters (adjust based on data size and needs)
INDEX_METRIC_TYPE = "L2" # Or "IP"
INDEX_TYPE = "IVF_FLAT"  # Milvus Lite æ”¯æŒçš„ç´¢å¼•ç±»å‹
# HNSW index params (adjust as needed)
INDEX_PARAMS = {"nlist": 128}
# HNSW search params (adjust as needed)
SEARCH_PARAMS = {"nprobe": 16}

# Generation Parameters
MAX_NEW_TOKENS_GEN = 512
TEMPERATURE = 0.7
TOP_P = 0.9
REPETITION_PENALTY = 1.1

# Global map to store document content (populated during indexing)
# Key: document ID (int), Value: dict {'title': str, 'abstract': str, 'content': str}
id_to_doc_map = {}
</file>

<file path="data_utils.py">
import json
import streamlit as st

def load_data(filepath):
    """Loads data from the JSON file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        st.write(f"Loaded {len(data)} articles from {filepath}")
        return data
    except FileNotFoundError:
        st.error(f"Data file not found: {filepath}")
        return []
    except json.JSONDecodeError:
        st.error(f"Error decoding JSON from file: {filepath}")
        return []
    except Exception as e:
        st.error(f"An error occurred loading data: {e}")
        return []
</file>

<file path="milvus_utils.py">
import streamlit as st
# Use MilvusClient for Lite version
from pymilvus import MilvusClient, DataType, CollectionSchema, FieldSchema
import time
import os

# Import config variables including the global map
from config import (
    MILVUS_LITE_DATA_PATH, COLLECTION_NAME, EMBEDDING_DIM,
    MAX_ARTICLES_TO_INDEX, INDEX_METRIC_TYPE, INDEX_TYPE, INDEX_PARAMS,
    SEARCH_PARAMS, TOP_K, id_to_doc_map
)

@st.cache_resource
def get_milvus_client():
    """Initializes and returns a MilvusClient instance for Milvus Lite."""
    try:
        st.write(f"Initializing Milvus Lite client with data path: {MILVUS_LITE_DATA_PATH}")
        # Ensure the directory for the data file exists
        os.makedirs(os.path.dirname(MILVUS_LITE_DATA_PATH), exist_ok=True)
        # The client connects to the local file specified
        client = MilvusClient(uri=MILVUS_LITE_DATA_PATH)
        st.success("Milvus Lite client initialized!")
        return client
    except Exception as e:
        st.error(f"Failed to initialize Milvus Lite client: {e}")
        return None

@st.cache_resource
def setup_milvus_collection(_client):
    """Ensures the specified collection exists and is set up correctly in Milvus Lite."""
    if not _client:
        st.error("Milvus client not available.")
        return False
    try:
        collection_name = COLLECTION_NAME
        dim = EMBEDDING_DIM

        has_collection = collection_name in _client.list_collections()

        if not has_collection:
            st.write(f"Collection '{collection_name}' not found. Creating...")
            # Define fields using new API style if needed (older style might still work)
            fields = [
                FieldSchema(name="id", dtype=DataType.INT64, is_primary=True),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dim),
                # You can add other scalar fields directly here for storage
                FieldSchema(name="content_preview", dtype=DataType.VARCHAR, max_length=500), # Example
            ]
            schema = CollectionSchema(fields, f"PubMed Lite RAG (dim={dim})")

            _client.create_collection(
                collection_name=collection_name,
                schema=schema # Pass schema directly or define dimension/primary field name
                # Or simpler:
                # dimension=dim,
                # primary_field_name="id",
                # vector_field_name="embedding",
                # metric_type=INDEX_METRIC_TYPE
            )
            st.write(f"Collection '{collection_name}' created.")

            # Create an index
            st.write(f"Creating index ({INDEX_TYPE})...")
            index_params = _client.prepare_index_params()
            index_params.add_index(
                field_name="embedding",
                index_type=INDEX_TYPE,
                metric_type=INDEX_METRIC_TYPE,
                params=INDEX_PARAMS
            )
            _client.create_index(collection_name, index_params)
            st.success(f"Index created for collection '{collection_name}'.")
        else:
            st.write(f"Found existing collection: '{collection_name}'.")
            # Optional: Check schema compatibility if needed

        # Determine current entity count (fallback between num_entities and stats)
        try:
            if hasattr(_client, 'num_entities'):
                current_count = _client.num_entities(collection_name)
            else:
                stats = _client.get_collection_stats(collection_name)
                current_count = int(stats.get("row_count", stats.get("rowCount", 0)))
            st.write(f"Collection '{collection_name}' ready. Current entity count: {current_count}")
        except Exception:
            st.write(f"Collection '{collection_name}' ready.")

        return True # Indicate collection is ready

    except Exception as e:
        st.error(f"Error setting up Milvus collection '{COLLECTION_NAME}': {e}")
        return False


def index_data_if_needed(client, data, embedding_model):
    """Checks if data needs indexing and performs it using MilvusClient."""
    global id_to_doc_map # Modify the global map

    if not client:
        st.error("Milvus client not available for indexing.")
        return False

    collection_name = COLLECTION_NAME
    # Retrieve current entity count with fallback
    try:
        if hasattr(client, 'num_entities'):
            current_count = client.num_entities(collection_name)
        else:
            stats = client.get_collection_stats(collection_name)
            current_count = int(stats.get("row_count", stats.get("rowCount", 0)))
    except Exception:
        st.write(f"Could not retrieve entity count, attempting to (re)setup collection.")
        if not setup_milvus_collection(client):
            return False
        current_count = 0  # Assume empty after setup

    st.write(f"Entities currently in Milvus collection '{collection_name}': {current_count}")

    data_to_index = data[:MAX_ARTICLES_TO_INDEX] # Limit data for demo
    needed_count = 0
    docs_for_embedding = []
    data_to_insert = [] # List of dictionaries for MilvusClient insert
    temp_id_map = {} # Build a temporary map first

    # Prepare data
    with st.spinner("Preparing data for indexing..."):
        for i, doc in enumerate(data_to_index):
             title = doc.get('title', '') or ""
             abstract = doc.get('abstract', '') or ""
             content = f"Title: {title}\nAbstract: {abstract}".strip()
             if not content:
                 continue

             doc_id = i # Use list index as ID
             needed_count += 1
             temp_id_map[doc_id] = {
                 'title': title, 'abstract': abstract, 'content': content
             }
             docs_for_embedding.append(content)
             # Prepare data in dict format for MilvusClient
             data_to_insert.append({
                 "id": doc_id,
                 "embedding": None, # Placeholder, will be filled after encoding
                 "content_preview": content[:500] # Store preview if field exists
             })


    if current_count < needed_count and docs_for_embedding:
        st.warning(f"Indexing required ({current_count}/{needed_count} documents found). This may take a while...")

        st.write(f"Embedding {len(docs_for_embedding)} documents...")
        with st.spinner("Generating embeddings..."):
            start_embed = time.time()
            embeddings = embedding_model.encode(docs_for_embedding, show_progress_bar=True)
            end_embed = time.time()
            st.write(f"Embedding took {end_embed - start_embed:.2f} seconds.")

        # Fill in the embeddings
        for i, emb in enumerate(embeddings):
            data_to_insert[i]["embedding"] = emb

        st.write("Inserting data into Milvus Lite...")
        with st.spinner("Inserting..."):
            try:
                start_insert = time.time()
                # MilvusClient uses insert() with list of dicts
                res = client.insert(collection_name=collection_name, data=data_to_insert)
                # Milvus Lite might automatically flush or sync, explicit flush isn't usually needed/available
                end_insert = time.time()
                # ä½¿ç”¨ len(data_to_insert) ä½œä¸ºæˆåŠŸæ’å…¥çš„æ•°é‡ï¼Œå› ä¸º res å¯èƒ½æ²¡æœ‰ primary_keys å±æ€§
                inserted_count = len(data_to_insert)
                st.success(f"Successfully attempted to index {inserted_count} documents. Insert took {end_insert - start_insert:.2f} seconds.")
                # Update the global map ONLY after successful insertion attempt
                id_to_doc_map.update(temp_id_map)
                return True
            except Exception as e:
                st.error(f"Error inserting data into Milvus Lite: {e}")
                return False
    elif current_count >= needed_count:
        st.write("Data count suggests indexing is complete.")
        # Populate the global map if it's empty but indexing isn't needed
        if not id_to_doc_map:
            id_to_doc_map.update(temp_id_map)
        return True
    else: # No docs_for_embedding found
         st.error("No valid text content found in the data to index.")
         return False


def search_similar_documents(client, query, embedding_model):
    """Searches Milvus Lite for documents similar to the query using MilvusClient."""
    if not client or not embedding_model:
        st.error("Milvus client or embedding model not available for search.")
        return [], []

    collection_name = COLLECTION_NAME
    try:
        query_embedding = embedding_model.encode([query])[0]

        # é‡å†™searchè°ƒç”¨ï¼Œä½¿ç”¨æ›´å…¼å®¹çš„æ–¹å¼
        search_params = {
            "collection_name": collection_name,
            "data": [query_embedding],
            "anns_field": "embedding",
            "limit": TOP_K,
            "output_fields": ["id"]
        }
        
        # å°è¯•ä¸åŒçš„æ–¹å¼ä¼ é€’æœç´¢å‚æ•°
        if hasattr(client, 'search_with_params'):
            # å¦‚æœå­˜åœ¨ä¸“é—¨çš„æ–¹æ³•
            res = client.search_with_params(**search_params, search_params=SEARCH_PARAMS)
        else:
            # æ ‡å‡†æ–¹æ³•ï¼Œç›´æ¥è®¾ç½®å‚æ•°ï¼ˆå½“å‰ç‰ˆæœ¬ä¼šå¯¼è‡´å‚æ•°å†²çªï¼‰
            try:
                # å°è¯•1ï¼šä¸ä¼ é€’paramå‚æ•°
                res = client.search(**search_params)
            except Exception as e1:
                st.warning(f"æœç´¢å°è¯•1å¤±è´¥: {e1}ï¼Œå°†å°è¯•å¤‡ç”¨æ–¹æ³•...")
                try:
                    # å°è¯•2ï¼šé€šè¿‡æœç´¢å‚æ•°å…³é”®å­—ä¼ é€’
                    res = client.search(**search_params, **SEARCH_PARAMS)
                except Exception as e2:
                    st.warning(f"æœç´¢å°è¯•2å¤±è´¥: {e2}ï¼Œå°†å°è¯•æœ€åä¸€ç§æ–¹æ³•...")
                    # å°è¯•3ï¼šç»“åˆå‚æ•°
                    final_params = search_params.copy()
                    final_params["nprobe"] = SEARCH_PARAMS.get("nprobe", 16)
                    res = client.search(**final_params)

        # Process results (structure might differ slightly)
        # client.search returns a list of lists of hits (one list per query vector)
        if not res or not res[0]:
            return [], []

        hit_ids = [hit['id'] for hit in res[0]]
        distances = [hit['distance'] for hit in res[0]]
        return hit_ids, distances
    except Exception as e:
        st.error(f"Error during Milvus Lite search: {e}")
        return [], []
</file>

<file path="models.py">
import streamlit as st
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

@st.cache_resource
def load_embedding_model(model_name):
    """Loads the sentence transformer model."""
    st.write(f"Loading embedding model: {model_name}...")
    try:
        model = SentenceTransformer(model_name)
        st.success("Embedding model loaded.")
        return model
    except Exception as e:
        st.error(f"Failed to load embedding model: {e}")
        return None

@st.cache_resource
def load_generation_model(model_name):
    """Loads the Hugging Face generative model and tokenizer."""
    st.write(f"Loading generation model: {model_name}...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        # Adjust device_map and torch_dtype based on your hardware
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            device_map="auto", # Use 'cpu' if no GPU or driver issues
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )
        if tokenizer.pad_token is None:
             tokenizer.pad_token = tokenizer.eos_token
        st.success("Generation model and tokenizer loaded.")
        return model, tokenizer
    except Exception as e:
        st.error(f"Failed to load generation model: {e}")
        return None, None
</file>

<file path="preprocess.py">
import os
import json
from bs4 import BeautifulSoup
import re

def extract_text_and_title_from_html(html_filepath):
    """
    ä»æŒ‡å®šçš„ HTML æ–‡ä»¶ä¸­æå–æ ‡é¢˜å’Œæ­£æ–‡æ–‡æœ¬ã€‚

    Args:
        html_filepath (str): HTML æ–‡ä»¶çš„è·¯å¾„ã€‚

    Returns:
        tuple: (æ ‡é¢˜, æ­£æ–‡æ–‡æœ¬) æˆ– (None, None) å¦‚æœå¤±è´¥ã€‚
    """
    try:
        with open(html_filepath, 'r', encoding='utf-8') as f:
            html_content = f.read()

        soup = BeautifulSoup(html_content, 'lxml') # æˆ–è€…ä½¿ç”¨ 'html.parser'

        # --- æå–æ ‡é¢˜ ---
        title_tag = soup.find('title')
        title_string = title_tag.string if title_tag else None
        # ç¡®ä¿ title_string ä¸ä¸º None æ‰è°ƒç”¨ strip()
        title = title_string.strip() if title_string else os.path.basename(html_filepath)
        title = title.replace('.html', '') # æ¸…ç†æ ‡é¢˜

        # --- å®šä½æ­£æ–‡å†…å®¹ ---
        # æ ¹æ®ä¹‹å‰çš„è®¨è®ºï¼Œä¼˜å…ˆæŸ¥æ‰¾ <content> æˆ–ç‰¹å®š class
        content_tag = soup.find('content')
        if not content_tag:
            content_tag = soup.find('div', class_='rich_media_content') # å¾®ä¿¡æ–‡ç« å¸¸è§
        if not content_tag:
            content_tag = soup.find('article') # HTML5 è¯­ä¹‰æ ‡ç­¾
        if not content_tag:
            content_tag = soup.find('main') # HTML5 è¯­ä¹‰æ ‡ç­¾
        if not content_tag:
             content_tag = soup.find('body') # æœ€åå°è¯• body

        if content_tag:
            # è·å–æ–‡æœ¬ï¼Œå°è¯•ä¿ç•™æ®µè½æ¢è¡Œç¬¦
            text = content_tag.get_text(separator='\n', strip=True)
            # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
            text = re.sub(r'\n\s*\n', '\n', text).strip()
            # å¯é€‰ï¼šè¿›ä¸€æ­¥æ¸…ç†ç‰¹å®šæ¨¡å¼ï¼ˆå¦‚å¹¿å‘Šã€é¡µè„šç­‰ï¼‰
            text = text.replace('é˜…è¯»åŸæ–‡', '').strip()
            return title, text
        else:
            print(f"è­¦å‘Šï¼šåœ¨æ–‡ä»¶ {html_filepath} ä¸­æœªæ‰¾åˆ°æ˜ç¡®çš„æ­£æ–‡æ ‡ç­¾ã€‚")
            return title, None # è¿”å›æ ‡é¢˜ï¼Œä½†æ–‡æœ¬ä¸º None

    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ–‡ä»¶ {html_filepath} æœªæ‰¾åˆ°ã€‚")
        return None, None
    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶ {html_filepath} æ—¶å‡ºé”™: {e}")
        return None, None

def split_text(text, chunk_size=500, chunk_overlap=50):
    """
    å°†æ–‡æœ¬åˆ†å‰²æˆæŒ‡å®šå¤§å°çš„å—ï¼Œå¹¶å¸¦æœ‰é‡å ã€‚

    Args:
        text (str): è¦åˆ†å‰²çš„æ–‡æœ¬ã€‚
        chunk_size (int): æ¯ä¸ªå—çš„ç›®æ ‡å­—ç¬¦æ•°ã€‚
        chunk_overlap (int): ç›¸é‚»å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ã€‚

    Returns:
        list[str]: æ–‡æœ¬å—åˆ—è¡¨ã€‚
    """
    if not text:
        return []

    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - chunk_overlap
        if start >= len(text): # é¿å…æ— é™å¾ªç¯ï¼ˆå¦‚æœ overlap >= sizeï¼‰
             break
        # å¦‚æœæœ€åä¸€å—å¤ªçŸ­ï¼Œå¹¶ä¸”æœ‰é‡å ï¼Œå¯èƒ½å¯¼è‡´é‡å¤æ·»åŠ ï¼Œç¡®ä¿ä¸é‡å¤
        if start < chunk_size and len(chunks)>1 and chunks[-1] == chunks[-2][chunk_size-chunk_overlap:]:
             chunks.pop() # ç§»é™¤é‡å¤çš„å°å°¾å·´
             start = len(text) # å¼ºåˆ¶ç»“æŸ

    # å¤„ç†æœ€åä¸€å—å¯èƒ½ä¸è¶³ chunk_size çš„æƒ…å†µ
    if start < len(text) and start > 0 : # ç¡®ä¿ä¸æ˜¯ç¬¬ä¸€ä¸ªå—å°±å°äºsize
         last_chunk = text[start-chunk_size+chunk_overlap:]
         if chunks and last_chunk != chunks[-1]: # é¿å…é‡å¤æ·»åŠ å®Œå…¨ç›¸åŒçš„æœ€åä¸€å—
            # æ£€æŸ¥æ˜¯å¦å’Œä¸Šä¸€ä¸ªå—çš„å°¾éƒ¨é‡å¤å¤ªå¤š
            if not chunks[-1].endswith(last_chunk):
                 chunks.append(last_chunk)
         elif not chunks: # å¦‚æœæ˜¯å”¯ä¸€ä¸€å—ä¸”å°äºsize
             chunks.append(last_chunk)


    # æ›´ç®€æ´çš„å®ç° (å¯èƒ½éœ€è¦å¾®è°ƒç¡®ä¿è¾¹ç•Œæƒ…å†µ)
    # chunks = []
    # for i in range(0, len(text), chunk_size - chunk_overlap):
    #     chunk = text[i:i + chunk_size]
    #     if chunk: # ç¡®ä¿ä¸æ·»åŠ ç©ºå—
    #         chunks.append(chunk)
    # # ç¡®ä¿æœ€åä¸€éƒ¨åˆ†è¢«åŒ…å« (å¦‚æœä¸Šé¢æ­¥é•¿å¯¼è‡´é—æ¼)
    # if chunks and len(text) > (len(chunks) -1) * (chunk_size - chunk_overlap) + chunk_size :
    #      last_start = (len(chunks) -1) * (chunk_size - chunk_overlap)
    #      final_chunk = text[last_start:]
    #      if final_chunk != chunks[-1]: # é¿å…é‡å¤
    #          # å¯ä»¥è€ƒè™‘åˆå¹¶æœ€åä¸¤ä¸ªå—å¦‚æœæœ€åä¸€ä¸ªå¤ªå°ï¼Œæˆ–ç›´æ¥æ·»åŠ 
    #           if len(final_chunk) > chunk_overlap : # é¿å…æ·»åŠ å¤ªå°çš„é‡å éƒ¨åˆ†
    #               chunks.append(final_chunk[overlap:]) # åªæ·»åŠ æ–°çš„éƒ¨åˆ†? æˆ–è€…å®Œæ•´æ·»åŠ ? å–å†³äºéœ€æ±‚
    #               # ç®€å•èµ·è§ï¼Œå…ˆå®Œæ•´æ·»åŠ 
    #               chunks.append(text[last_start + chunk_size - chunk_overlap:])


    return [c.strip() for c in chunks if c.strip()] # è¿”å›éç©ºå—

# --- é…ç½® ---
html_directory = './data/' # **** ä¿®æ”¹ä¸ºä½ çš„ HTML æ–‡ä»¶å¤¹è·¯å¾„ ****
output_json_path = './data/processed_data.json' # **** è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ ****
CHUNK_SIZE = 512  # æ¯ä¸ªæ–‡æœ¬å—çš„ç›®æ ‡å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
CHUNK_OVERLAP = 50 # ç›¸é‚»æ–‡æœ¬å—çš„é‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰

# --- ä¸»å¤„ç†é€»è¾‘ ---
all_data_for_milvus = []
file_count = 0
chunk_count = 0

print(f"å¼€å§‹å¤„ç†ç›®å½• '{html_directory}' ä¸­çš„ HTML æ–‡ä»¶...")

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(os.path.dirname(output_json_path), exist_ok=True)

html_files = [f for f in os.listdir(html_directory) if f.endswith('.html')]
print(f"æ‰¾åˆ° {len(html_files)} ä¸ª HTML æ–‡ä»¶ã€‚")

for filename in html_files:
    filepath = os.path.join(html_directory, filename)
    print(f"  å¤„ç†æ–‡ä»¶: {filename} ...")
    file_count += 1

    title, main_text = extract_text_and_title_from_html(filepath)

    if main_text:
        chunks = split_text(main_text, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
        print(f"    æå–åˆ°æ–‡æœ¬ï¼Œåˆ†å‰²æˆ {len(chunks)} ä¸ªå—ã€‚")

        for i, chunk in enumerate(chunks):
            chunk_count += 1
            # æ„å»ºç¬¦åˆ milvus_utils.py æœŸæœ›çš„å­—å…¸ç»“æ„
            milvus_entry = {
                "id": f"{filename}_{i}", # åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„ ID (æ–‡ä»¶å + å—ç´¢å¼•)
                "title": title or filename, # ä½¿ç”¨æå–çš„æ ‡é¢˜æˆ–æ–‡ä»¶å
                "abstract": chunk, # å°†æ–‡æœ¬å—æ”¾å…¥ 'abstract' å­—æ®µ
                "source_file": filename, # æ·»åŠ åŸå§‹æ–‡ä»¶åä»¥ä¾›å‚è€ƒ
                "chunk_index": i
            }
            all_data_for_milvus.append(milvus_entry)
    else:
        print(f"    è­¦å‘Šï¼šæœªèƒ½ä» {filename} æå–æœ‰æ•ˆæ–‡æœ¬å†…å®¹ã€‚")

print(f"\nå¤„ç†å®Œæˆã€‚å…±å¤„ç† {file_count} ä¸ªæ–‡ä»¶ï¼Œç”Ÿæˆ {chunk_count} ä¸ªæ–‡æœ¬å—ã€‚")

# --- ä¿å­˜ä¸º JSON ---
try:
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(all_data_for_milvus, f, ensure_ascii=False, indent=4)
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_json_path}")
except Exception as e:
    print(f"é”™è¯¯ï¼šæ— æ³•å†™å…¥ JSON æ–‡ä»¶ {output_json_path}: {e}")
</file>

<file path="rag_core.py">
import streamlit as st
import torch
from config import MAX_NEW_TOKENS_GEN, TEMPERATURE, TOP_P, REPETITION_PENALTY

def generate_answer(query, context_docs, gen_model, tokenizer):
    """Generates an answer using the LLM based on query and context."""
    if not context_docs:
        return "I couldn't find relevant documents to answer your question."
    if not gen_model or not tokenizer:
         st.error("Generation model or tokenizer not available.")
         return "Error: Generation components not loaded."

    context = "\n\n---\n\n".join([doc['content'] for doc in context_docs]) # Combine retrieved docs

    prompt = f"""Based ONLY on the following context documents, answer the user's question.
If the answer is not found in the context, state that clearly. Do not make up information.

Context Documents:
{context}

User Question: {query}

Answer:
"""
    try:
        inputs = tokenizer(prompt, return_tensors="pt").to(gen_model.device)
        with torch.no_grad():
            outputs = gen_model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS_GEN,
                temperature=TEMPERATURE,
                top_p=TOP_P,
                repetition_penalty=REPETITION_PENALTY,
                pad_token_id=tokenizer.eos_token_id # Important for open-end generation
            )
        # Decode only the newly generated tokens, excluding the prompt
        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        return response.strip()
    except Exception as e:
        st.error(f"Error during text generation: {e}")
        return "Sorry, I encountered an error while generating the answer."
</file>

</files>
